{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: \n",
    "Cart Position: -0.00713985 (close to the center of the track).\n",
    "Cart Velocity: -0.04509013 (the cart is moving slightly to the left).\n",
    "Pole Angle: 0.01458127 (the pole is nearly upright).\n",
    "Pole Velocity at Tip: 0.04600234 (the pole is tilting to one side at a small rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.02323877, -0.00714208,  0.04272482,  0.04248186], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env.step(1) returns observation, reward, done, _, info\n",
    "# Return observation after performing action 1 (move cart to the right), not done yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeural(nn.Module):\n",
    "    def __init__(self, n_actions, input_dim, hidden_dim):\n",
    "        super(DeepNeural, self).__init__()\n",
    "        self.action = n_actions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, n_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        #state is also observation: input_dim = 4\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        return actions #4 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepNeural(2, 4, 128)\n",
    "state = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "model.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a neural net that take inputs as actions and return output\n",
    "class Agent:\n",
    "    def __init__(self, epsilon, discount_factor, lr, input_dims, hidden_dim, batch_size, n_actions, memory_size = 10000):\n",
    "        #input_dims = observation or state number of values\n",
    "        self.actions_space = [i for i in range(n_actions)] #[0, 1], we want to use to randomly choose action later\n",
    "        self.batch_size = batch_size\n",
    "        self.current_memory = 0\n",
    "        self.memory_size = memory_size\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        self.state_memory = np.zeros((memory_size, input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((memory_size, input_dims), dtype=np.float32)\n",
    "        self.reward_memory = np.zeros(memory_size, dtype=np.int32)\n",
    "        self.action_memory = np.zeros(memory_size, dtype=np.int32)\n",
    "        self.terminal_memory = np.zeros(memory_size, dtype=bool) #[True, True, False, False....]\n",
    "        self.Q_net = DeepNeural(n_actions, input_dims, hidden_dim)\n",
    "        self.optimizer = optim.Adam(self.Q_net.parameters(), lr = lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "    \n",
    "    def store_action(self, state, action, new_state, reward, done):\n",
    "        '''Purpose: we want to store the action per time to the memory'''\n",
    "        idx = self.current_memory % self.memory_size\n",
    "        self.state_memory[idx] = state\n",
    "        self.new_state_memory[idx] = new_state\n",
    "        self.action_memory[idx] = action #[0, 1, 1, 0, 1, 0, 1,....]\n",
    "        self.reward_memory[idx] = reward\n",
    "        self.terminal_memory[idx] = done #[False, True, false, false....]\n",
    "        self.current_memory += 1\n",
    "        \n",
    "    def choose_action(self, state): #given the state, how to choose action to perform the next step\n",
    "        if np.random.random() > self.epsilon: #epsilon is about 0.1\n",
    "            actions = self.Q_net(torch.tensor([state])) #return a list of actions, choose actions with highest probability\n",
    "            action = torch.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.actions_space)\n",
    "        return action\n",
    "\n",
    "    def learn(self): #loop through every batch and learn\n",
    "        if self.current_memory < self.batch_size:\n",
    "            return \n",
    "        self.optimizer.zero_grad()\n",
    "        max_mem = min(self.current_memory, self.memory_size)\n",
    "        a_batch = np.random.choice(max_mem, self.batch_size, replace = False)# choose random samples in max_mem with len = batch_size, no duplicate => (batch_size, )\n",
    "        #[mm3, mm1, mm65, mm2....]\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        state_batch = torch.tensor(self.state_memory[a_batch]) #batch_size x input_dims\n",
    "        new_state_batch = torch.tensor(self.new_state_memory[a_batch]) #batch_size x input_dims\n",
    "        reward_batch = torch.tensor(self.reward_memory[a_batch]) #batch_size\n",
    "        action_batch = torch.tensor(self.action_memory[a_batch], dtype=torch.int32) #batch_size\n",
    "        terminal_batch = torch.tensor(self.terminal_memory[a_batch]) #batch_size\n",
    "        \n",
    "        q_val = self.Q_net(state_batch)[batch_index, action_batch] # batch_size x 1\n",
    "        q_next = self.Q_net(new_state_batch)  #batch_size x n_actions -> batch_size x 2\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        q_target = reward_batch + self.discount_factor * torch.max(q_next, dim=1)[0] # batch_size\n",
    "        #torch.max(q_next, dim =1) returns a tuple of values and indices, we access values only by adding [0]\n",
    "        loss = self.loss(q_target, q_val)\n",
    "        loss.backward()\n",
    "        self.optimizer.step() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: in learn method, why set q_next[terminal_batch] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\tscore: 14.0\taverage score: 14.0\t epsilon: 0.1\n",
      "episode: 1\tscore: 10.0\taverage score: 12.0\t epsilon: 0.1\n",
      "episode: 2\tscore: 10.0\taverage score: 11.333333333333334\t epsilon: 0.1\n",
      "episode: 3\tscore: 10.0\taverage score: 11.0\t epsilon: 0.1\n",
      "episode: 4\tscore: 9.0\taverage score: 10.6\t epsilon: 0.1\n",
      "episode: 5\tscore: 9.0\taverage score: 10.333333333333334\t epsilon: 0.1\n",
      "episode: 6\tscore: 9.0\taverage score: 10.142857142857142\t epsilon: 0.1\n",
      "episode: 7\tscore: 9.0\taverage score: 10.0\t epsilon: 0.1\n",
      "episode: 8\tscore: 14.0\taverage score: 10.444444444444445\t epsilon: 0.1\n",
      "episode: 9\tscore: 12.0\taverage score: 10.6\t epsilon: 0.1\n",
      "episode: 10\tscore: 10.0\taverage score: 10.545454545454545\t epsilon: 0.1\n",
      "episode: 11\tscore: 9.0\taverage score: 10.416666666666666\t epsilon: 0.1\n",
      "episode: 12\tscore: 15.0\taverage score: 10.76923076923077\t epsilon: 0.1\n",
      "episode: 13\tscore: 11.0\taverage score: 10.785714285714286\t epsilon: 0.1\n",
      "episode: 14\tscore: 43.0\taverage score: 12.933333333333334\t epsilon: 0.1\n",
      "episode: 15\tscore: 59.0\taverage score: 15.8125\t epsilon: 0.1\n",
      "episode: 16\tscore: 143.0\taverage score: 23.294117647058822\t epsilon: 0.1\n",
      "episode: 17\tscore: 52.0\taverage score: 24.88888888888889\t epsilon: 0.1\n",
      "episode: 18\tscore: 62.0\taverage score: 26.842105263157894\t epsilon: 0.1\n",
      "episode: 19\tscore: 63.0\taverage score: 28.65\t epsilon: 0.1\n",
      "episode: 20\tscore: 70.0\taverage score: 30.61904761904762\t epsilon: 0.1\n",
      "episode: 21\tscore: 58.0\taverage score: 31.863636363636363\t epsilon: 0.1\n",
      "episode: 22\tscore: 88.0\taverage score: 34.30434782608695\t epsilon: 0.1\n",
      "episode: 23\tscore: 75.0\taverage score: 36.0\t epsilon: 0.1\n",
      "episode: 24\tscore: 81.0\taverage score: 37.8\t epsilon: 0.1\n",
      "episode: 25\tscore: 127.0\taverage score: 41.23076923076923\t epsilon: 0.1\n",
      "episode: 26\tscore: 11.0\taverage score: 40.111111111111114\t epsilon: 0.1\n",
      "episode: 27\tscore: 98.0\taverage score: 42.17857142857143\t epsilon: 0.1\n",
      "episode: 28\tscore: 83.0\taverage score: 43.58620689655172\t epsilon: 0.1\n",
      "episode: 29\tscore: 96.0\taverage score: 45.333333333333336\t epsilon: 0.1\n",
      "episode: 30\tscore: 115.0\taverage score: 47.58064516129032\t epsilon: 0.1\n",
      "episode: 31\tscore: 109.0\taverage score: 49.5\t epsilon: 0.1\n",
      "episode: 32\tscore: 105.0\taverage score: 51.18181818181818\t epsilon: 0.1\n",
      "episode: 33\tscore: 105.0\taverage score: 52.76470588235294\t epsilon: 0.1\n",
      "episode: 34\tscore: 127.0\taverage score: 54.885714285714286\t epsilon: 0.1\n",
      "episode: 35\tscore: 108.0\taverage score: 56.361111111111114\t epsilon: 0.1\n",
      "episode: 36\tscore: 113.0\taverage score: 57.891891891891895\t epsilon: 0.1\n",
      "episode: 37\tscore: 122.0\taverage score: 59.578947368421055\t epsilon: 0.1\n",
      "episode: 38\tscore: 123.0\taverage score: 61.205128205128204\t epsilon: 0.1\n",
      "episode: 39\tscore: 129.0\taverage score: 62.9\t epsilon: 0.1\n",
      "episode: 40\tscore: 114.0\taverage score: 64.14634146341463\t epsilon: 0.1\n",
      "episode: 41\tscore: 124.0\taverage score: 65.57142857142857\t epsilon: 0.1\n",
      "episode: 42\tscore: 123.0\taverage score: 66.90697674418605\t epsilon: 0.1\n",
      "episode: 43\tscore: 130.0\taverage score: 68.3409090909091\t epsilon: 0.1\n",
      "episode: 44\tscore: 152.0\taverage score: 70.2\t epsilon: 0.1\n",
      "episode: 45\tscore: 141.0\taverage score: 71.73913043478261\t epsilon: 0.1\n",
      "episode: 46\tscore: 143.0\taverage score: 73.25531914893617\t epsilon: 0.1\n",
      "episode: 47\tscore: 125.0\taverage score: 74.33333333333333\t epsilon: 0.1\n",
      "episode: 48\tscore: 156.0\taverage score: 76.0\t epsilon: 0.1\n",
      "episode: 49\tscore: 164.0\taverage score: 77.76\t epsilon: 0.1\n",
      "episode: 50\tscore: 153.0\taverage score: 79.23529411764706\t epsilon: 0.1\n",
      "episode: 51\tscore: 243.0\taverage score: 82.38461538461539\t epsilon: 0.1\n",
      "episode: 52\tscore: 161.0\taverage score: 83.86792452830188\t epsilon: 0.1\n",
      "episode: 53\tscore: 128.0\taverage score: 84.68518518518519\t epsilon: 0.1\n",
      "episode: 54\tscore: 181.0\taverage score: 86.43636363636364\t epsilon: 0.1\n",
      "episode: 55\tscore: 172.0\taverage score: 87.96428571428571\t epsilon: 0.1\n",
      "episode: 56\tscore: 139.0\taverage score: 88.85964912280701\t epsilon: 0.1\n",
      "episode: 57\tscore: 143.0\taverage score: 89.79310344827586\t epsilon: 0.1\n",
      "episode: 58\tscore: 143.0\taverage score: 90.69491525423729\t epsilon: 0.1\n",
      "episode: 59\tscore: 173.0\taverage score: 92.06666666666666\t epsilon: 0.1\n",
      "episode: 60\tscore: 163.0\taverage score: 93.22950819672131\t epsilon: 0.1\n",
      "episode: 61\tscore: 144.0\taverage score: 94.04838709677419\t epsilon: 0.1\n",
      "episode: 62\tscore: 168.0\taverage score: 95.22222222222223\t epsilon: 0.1\n",
      "episode: 63\tscore: 146.0\taverage score: 96.015625\t epsilon: 0.1\n",
      "episode: 64\tscore: 158.0\taverage score: 96.96923076923076\t epsilon: 0.1\n",
      "episode: 65\tscore: 180.0\taverage score: 98.22727272727273\t epsilon: 0.1\n",
      "episode: 66\tscore: 172.0\taverage score: 99.32835820895522\t epsilon: 0.1\n",
      "episode: 67\tscore: 141.0\taverage score: 99.94117647058823\t epsilon: 0.1\n",
      "episode: 68\tscore: 128.0\taverage score: 100.34782608695652\t epsilon: 0.1\n",
      "episode: 69\tscore: 136.0\taverage score: 100.85714285714286\t epsilon: 0.1\n",
      "episode: 70\tscore: 136.0\taverage score: 101.35211267605634\t epsilon: 0.1\n",
      "episode: 71\tscore: 204.0\taverage score: 102.77777777777777\t epsilon: 0.1\n",
      "episode: 72\tscore: 363.0\taverage score: 106.34246575342466\t epsilon: 0.1\n",
      "episode: 73\tscore: 158.0\taverage score: 107.04054054054055\t epsilon: 0.1\n",
      "episode: 74\tscore: 149.0\taverage score: 107.6\t epsilon: 0.1\n",
      "episode: 75\tscore: 63.0\taverage score: 107.01315789473684\t epsilon: 0.1\n",
      "episode: 76\tscore: 166.0\taverage score: 107.77922077922078\t epsilon: 0.1\n",
      "episode: 77\tscore: 216.0\taverage score: 109.16666666666667\t epsilon: 0.1\n",
      "episode: 78\tscore: 73.0\taverage score: 108.70886075949367\t epsilon: 0.1\n",
      "episode: 79\tscore: 165.0\taverage score: 109.4125\t epsilon: 0.1\n",
      "episode: 80\tscore: 187.0\taverage score: 110.37037037037037\t epsilon: 0.1\n",
      "episode: 81\tscore: 167.0\taverage score: 111.0609756097561\t epsilon: 0.1\n",
      "episode: 82\tscore: 307.0\taverage score: 113.42168674698796\t epsilon: 0.1\n",
      "episode: 83\tscore: 218.0\taverage score: 114.66666666666667\t epsilon: 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     14\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstore_action(observation, action, next_observation, reward, terminated)\n\u001b[1;32m---> 15\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     observation \u001b[38;5;241m=\u001b[39m next_observation\n\u001b[0;32m     17\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "Cell \u001b[1;32mIn[71], line 60\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(q_target, q_val)\n\u001b[0;32m     59\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\optim\\optimizer.py:469\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    468\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 469\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[0;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\profiler.py:688\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[1;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "agent = Agent(epsilon=0.1, discount_factor=0.8, lr = 0.001, batch_size=64, n_actions=2, input_dims = 4, hidden_dim=128, memory_size=10000)\n",
    "scores, eps_history = [], []\n",
    "games = 500\n",
    "for i in range(games):\n",
    "    score = 0\n",
    "    terminated = False\n",
    "    observation = env.reset() #return the initial values of obs\n",
    "    observation = observation[0]\n",
    "    while not terminated:\n",
    "        action = agent.choose_action(observation)\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_action(observation, action, next_observation, reward, terminated)\n",
    "        agent.learn()\n",
    "        observation = next_observation\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "    average_score = np.mean(scores[-100:])\n",
    "    print(f'episode: {i}\\tscore: {score}\\taverage score: {average_score}\\t epsilon: {agent.epsilon}')\n",
    "\n",
    "#Why do we want to know average score: the score in each episole can be up and down, but average score will tend to increase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
